**VeMa Twi Translator: Vision to Speech AI Project**

*Project Overview*

Our project, VeMa Twi Translator, is a web application designed to enhance accessibility by converting images to speech. When a user uploads an image, the application generates an English caption, translates the caption into Twi—a Ghanaian language—and provides an audio output of the Twi caption. This tool is particularly beneficial for the visually impaired and those who primarily understand Twi.


*Significance of the Project*

The significance of VeMa Twi Translator lies in its ability to bridge the communication gap for Twi speakers and provide an inclusive tool for the visually impaired. By choosing an African language, we aim to celebrate cultural diversity and address the underrepresentation of African languages in technology. Twi was selected due to its widespread use in Ghana, making our application valuable for a large number of native speakers.


*Vision to Speech Approach*

We chose the vision to speech approach because it offers a comprehensive solution for accessibility. By converting visual information into audio, our application caters to individuals who rely on auditory inputs. This method ensures that users can understand visual content without needing to see it.


**How It Works**

1.	Image Upload: Users can upload images in formats like JPEG or JPG. The acceptable formats are clearly specified on the upload interface.
2.	English Caption Generation: Once the image is uploaded, the application uses a pretrained model to generate an accurate English caption.
3.	Translation to Twi: The English caption is then translated into Twi using an open-source translation API.
4.	Twi Audio Output: Finally, the translated Twi caption is converted to speech, providing an audio output for the user.

   
**Implementation Details**

1.	Datasets:
   
o	Flickr8k Dataset: Used to test the accuracy of our model. This dataset contains images and corresponding captions, which help validate our image captioning model. https://www.kaggle.com/datasets/shadabhussain/flickr8k?select=Flickr_Data

o	Salesforce/BLIP-Image-Captioning-Base: A pretrained model from hugging face used for generating captions from images. We compared its output with the captions in the Flickr8k dataset to ensure accuracy. model_id = "Salesforce/blip-image-captioning-base"

o	Kakao-Enterprise/VITS-1JS: A pretrained model that converts text to speech. model="kakao-enterprise/vits-ljs"

2.	Translation API:

o	Free Translator API: An open-source API used for translating English captions to Twi. To utilize this API, we downloaded Golang and Ngrok. Golang is necessary to run the API, and Ngrok generates a URL to expose your local server to the internet and facilitate translation process . https://github.com/ismalzikri/free-translate-api

3.	Twi Text to Speech:

o	TT_Models/Tw_Asante/OpenBible/VITS: This model converts the translated Twi text into speech, providing the final audio output. https://aimodels.org/ai-models/text-to-speech-synthesis/twi_asante-male-tts-model-vits-encoding-trained-on-openbible-dataset-at-22050hz/

**Cloud Deployment**

To deploy our application on a cloud platform, follow these steps:

1.	Download Golang: Necessary for running the Free Translator API.
2.	Download Ngrok: Generates a URL required for the translation process.
3.	Create a Streamlit Account: If you don't have one, create an account on Streamlit.
4.	**Run the Application:**

    o	Copy the URL generated by Ngrok.
    
    o	Run the application on Streamlit using the copied URL.
    
    o	Upload an image in the specified format.
    
    o	The application will generate an English caption, translate it to Twi, and produce an audio output of the Twi 
      caption.

By following these steps, users can easily deploy and run VeMa Translator on their local machines, enhancing accessibility and communication for Twi speakers and the visually impaired.

